import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
import torchvision.models as models
import cv2
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os
from PIL import Image
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, r2_score
import warnings
from typing import Tuple, Dict, List, Optional
from dataclasses import dataclass
import albumentations as A
from albumentations.pytorch import ToTensorV2
import json
from tqdm import tqdm
import shutil
from datetime import datetime

warnings.filterwarnings('ignore')

@dataclass
class BoneAgeResult:
    """Results from bone age prediction"""
    predicted_age_months: float
    confidence: float
    age_range_min: float
    age_range_max: float
    uncertainty: float

class BoneAgeDataset(Dataset):
    """Dataset class for bone age X-ray images"""
    
    def __init__(self, image_paths: List[str], ages_months: List[float], 
                 genders: List[int], transform=None, is_training=True):
        self.image_paths = image_paths
        self.ages_months = ages_months
        self.genders = genders  # 0 for female, 1 for male
        self.transform = transform
        self.is_training = is_training
        
        # Create augmentation pipeline
        if is_training:
            self.aug_transform = A.Compose([
                A.Resize(512, 512),
                A.Rotate(limit=15, p=0.5),
                A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),
                A.GaussianBlur(blur_limit=3, p=0.3),
                A.HorizontalFlip(p=0.5),
                A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=10, p=0.5),
                A.Normalize(mean=[0.485], std=[0.229]),
                ToTensorV2()
            ])
        else:
            self.aug_transform = A.Compose([
                A.Resize(512, 512),
                A.Normalize(mean=[0.485], std=[0.229]),
                ToTensorV2()
            ])
    
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        # Load image
        image_path = self.image_paths[idx]
        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
        
        if image is None:
            raise ValueError(f"Could not load image: {image_path}")
        
        # Convert to 3-channel for pretrained models
        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)
        
        # Apply augmentations
        if self.aug_transform:
            augmented = self.aug_transform(image=image)
            image = augmented['image']
        
        # Get labels
        age_months = torch.tensor(self.ages_months[idx], dtype=torch.float32)
        gender = torch.tensor(self.genders[idx], dtype=torch.float32)
        
        return {
            'image': image,
            'age_months': age_months,
            'gender': gender,
            'image_path': image_path
        }

class BoneAgeModel(nn.Module):
    """Multi-task neural network for bone age and gender prediction"""
    
    def __init__(self, backbone='efficientnet_b3', pretrained=True, dropout=0.3):
        super(BoneAgeModel, self).__init__()
        
        # Load backbone
        if backbone == 'efficientnet_b3':
            from torchvision.models import efficientnet_b3
            self.backbone = efficientnet_b3(pretrained=pretrained)
            feature_dim = self.backbone.classifier[1].in_features
            self.backbone.classifier = nn.Identity()
        elif backbone == 'resnet50':
            self.backbone = models.resnet50(pretrained=pretrained)
            feature_dim = self.backbone.fc.in_features
            self.backbone.fc = nn.Identity()
        elif backbone == 'densenet121':
            self.backbone = models.densenet121(pretrained=pretrained)
            feature_dim = self.backbone.classifier.in_features
            self.backbone.classifier = nn.Identity()
        else:
            raise ValueError(f"Unsupported backbone: {backbone}")
        
        # Feature processing layers
        self.feature_processor = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(feature_dim, 512),
            nn.ReLU(),
            nn.BatchNorm1d(512),
            nn.Dropout(dropout/2),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.BatchNorm1d(256)
        )
        
        # Age regression head
        self.age_head = nn.Sequential(
            nn.Dropout(dropout/2),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, 1)
        )
        
        # Gender classification head (auxiliary task)
        self.gender_head = nn.Sequential(
            nn.Dropout(dropout/2),
            nn.Linear(256, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )
        
        # Uncertainty estimation head
        self.uncertainty_head = nn.Sequential(
            nn.Dropout(dropout/2),
            nn.Linear(256, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Softplus()  # Ensures positive uncertainty
        )
    
    def forward(self, x):
        # Extract features
        features = self.backbone(x)
        processed_features = self.feature_processor(features)
        
        # Predictions
        age_pred = self.age_head(processed_features)
        gender_pred = self.gender_head(processed_features)
        uncertainty = self.uncertainty_head(processed_features)
        
        return {
            'age': age_pred.squeeze(),
            'gender': gender_pred.squeeze(),
            'uncertainty': uncertainty.squeeze()
        }

class BoneAgeTrainer:
    """Training and evaluation class for bone age model"""
    
    def __init__(self, model, device='cuda'):
        self.model = model.to(device)
        self.device = device
        self.train_losses = []
        self.val_losses = []
        self.val_maes = []
    
    def load_checkpoint(self, checkpoint_path: str):
        """Load a previous checkpoint to resume training"""
        if os.path.exists(checkpoint_path):
            print(f"üìÇ Loading checkpoint from {checkpoint_path}")
            # FIX: Add weights_only=False for PyTorch 2.6+ compatibility
            checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=False)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            
            # Load training history if available
            if 'train_losses' in checkpoint:
                self.train_losses = checkpoint['train_losses']
            if 'val_losses' in checkpoint:
                self.val_losses = checkpoint['val_losses']
            if 'val_maes' in checkpoint:
                self.val_maes = checkpoint['val_maes']
                
            return checkpoint
        else:
            print(f"‚ö†Ô∏è No checkpoint found at {checkpoint_path}, starting from scratch")
            return None
    
    def save_checkpoint(self, checkpoint_path: str, epoch: int, val_mae: float, 
                       optimizer, scheduler, is_best=False):
        """Save model checkpoint with comprehensive state"""
        checkpoint = {
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict(),
            'epoch': epoch,
            'val_mae': val_mae,
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'val_maes': self.val_maes,
            'timestamp': datetime.now().isoformat(),
            'device': str(self.device)
        }
        
        torch.save(checkpoint, checkpoint_path)
        
        if is_best:
            # Also save as best model
            best_path = checkpoint_path.replace('.pth', '_best.pth')
            shutil.copy2(checkpoint_path, best_path)
            
            # Also save to root directory for easy access
            root_best_path = 'best_bone_age_model.pth'
            shutil.copy2(checkpoint_path, root_best_path)
            print(f"üíæ New best model saved! MAE: {val_mae:.2f} months")
            print(f"üìÇ Model also saved as: {root_best_path}")
        
    def train_epoch(self, train_loader, optimizer, criterion_age, criterion_gender, 
                   age_weight=1.0, gender_weight=0.3, uncertainty_weight=0.1):
        """Train for one epoch"""
        self.model.train()
        total_loss = 0
        total_age_loss = 0
        total_gender_loss = 0
        total_uncertainty_loss = 0
        
        pbar = tqdm(train_loader, desc="Training")
        for batch in pbar:
            images = batch['image'].to(self.device)
            ages = batch['age_months'].to(self.device)
            genders = batch['gender'].to(self.device)
            
            optimizer.zero_grad()
            
            # Forward pass
            outputs = self.model(images)
            
            # Calculate losses
            age_loss = criterion_age(outputs['age'], ages)
            gender_loss = criterion_gender(outputs['gender'], genders)
            
            # Uncertainty loss (encourage reasonable uncertainty estimates)
            uncertainty_loss = torch.mean(outputs['uncertainty'])
            
            # Combined loss
            total_loss_batch = (age_weight * age_loss + 
                              gender_weight * gender_loss + 
                              uncertainty_weight * uncertainty_loss)
            
            # Backward pass
            total_loss_batch.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            optimizer.step()
            
            # Update running losses
            total_loss += total_loss_batch.item()
            total_age_loss += age_loss.item()
            total_gender_loss += gender_loss.item()
            total_uncertainty_loss += uncertainty_loss.item()
            
            # Update progress bar
            pbar.set_postfix({
                'Loss': f'{total_loss_batch.item():.4f}',
                'Age MAE': f'{age_loss.item():.2f}',
                'Gender': f'{gender_loss.item():.4f}'
            })
        
        avg_loss = total_loss / len(train_loader)
        self.train_losses.append(avg_loss)
        
        return {
            'total_loss': avg_loss,
            'age_loss': total_age_loss / len(train_loader),
            'gender_loss': total_gender_loss / len(train_loader),
            'uncertainty_loss': total_uncertainty_loss / len(train_loader)
        }
    
    def validate(self, val_loader, criterion_age, criterion_gender):
        """Validate the model"""
        self.model.eval()
        total_loss = 0
        age_predictions = []
        age_targets = []
        gender_predictions = []
        gender_targets = []
        
        with torch.no_grad():
            for batch in tqdm(val_loader, desc="Validating"):
                images = batch['image'].to(self.device)
                ages = batch['age_months'].to(self.device)
                genders = batch['gender'].to(self.device)
                
                outputs = self.model(images)
                
                # Calculate losses
                age_loss = criterion_age(outputs['age'], ages)
                gender_loss = criterion_gender(outputs['gender'], genders)
                total_loss += (age_loss + 0.3 * gender_loss).item()
                
                # Store predictions for metrics
                age_predictions.extend(outputs['age'].cpu().numpy())
                age_targets.extend(ages.cpu().numpy())
                gender_predictions.extend(outputs['gender'].cpu().numpy())
                gender_targets.extend(genders.cpu().numpy())
        
        avg_loss = total_loss / len(val_loader)
        age_mae = mean_absolute_error(age_targets, age_predictions)
        age_r2 = r2_score(age_targets, age_predictions)
        
        gender_accuracy = np.mean((np.array(gender_predictions) > 0.5) == np.array(gender_targets))
        
        self.val_losses.append(avg_loss)
        self.val_maes.append(age_mae)
        
        return {
            'val_loss': avg_loss,
            'age_mae': age_mae,
            'age_r2': age_r2,
            'gender_accuracy': gender_accuracy,
            'predictions': age_predictions,
            'targets': age_targets
        }
    
    def fit(self, train_loader, val_loader, epochs=50, learning_rate=1e-4, 
            weight_decay=1e-5, patience=10, resume_from=None, model_name='bone_age_model'):
        """Train the model with early stopping and optional resume capability"""
        
        # Setup optimizer and criteria
        optimizer = optim.AdamW(self.model.parameters(), lr=learning_rate, weight_decay=weight_decay)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)
        
        criterion_age = nn.L1Loss()  # MAE for age regression
        criterion_gender = nn.BCELoss()  # Binary cross-entropy for gender
        
        best_val_mae = float('inf')
        patience_counter = 0
        start_epoch = 0
        last_epoch = start_epoch
        
        # Create model directory
        model_dir = f'models/{model_name}'
        os.makedirs(model_dir, exist_ok=True)
        
        # Resume from checkpoint if provided
        if resume_from and os.path.exists(resume_from):
            checkpoint = self.load_checkpoint(resume_from)
            if checkpoint:
                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                if 'scheduler_state_dict' in checkpoint:
                    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
                start_epoch = checkpoint['epoch'] + 1
                best_val_mae = checkpoint['val_mae']
                print(f"‚úÖ Resuming from epoch {start_epoch}, best MAE: {best_val_mae:.2f} months")
        
        print(f"üöÄ Starting training for {epochs} epochs (from epoch {start_epoch})...")
        print(f"üìÅ Model checkpoints will be saved to: {model_dir}")
        
        for epoch in range(start_epoch, start_epoch + epochs):
            last_epoch = epoch  # Track the last completed epoch
            print(f"\nEpoch {epoch + 1}/{start_epoch + epochs}")
            print("-" * 30)
            
            # Train
            train_metrics = self.train_epoch(train_loader, optimizer, criterion_age, criterion_gender)
            
            # Validate
            val_metrics = self.validate(val_loader, criterion_age, criterion_gender)
            
            # Learning rate scheduling
            scheduler.step(val_metrics['val_loss'])
            
            # Print metrics
            print(f"Train Loss: {train_metrics['total_loss']:.4f}")
            print(f"Val Loss: {val_metrics['val_loss']:.4f}")
            print(f"Val Age MAE: {val_metrics['age_mae']:.2f} months")
            print(f"Val Age R¬≤: {val_metrics['age_r2']:.4f}")
            print(f"Val Gender Acc: {val_metrics['gender_accuracy']:.4f}")
            print(f"Current LR: {optimizer.param_groups[0]['lr']:.6f}")
            
            # Save regular checkpoint
            checkpoint_path = f'{model_dir}/checkpoint_epoch_{epoch+1}.pth'
            is_best = val_metrics['age_mae'] < best_val_mae
            
            self.save_checkpoint(checkpoint_path, epoch, val_metrics['age_mae'], 
                               optimizer, scheduler, is_best)
            
            # Early stopping
            if is_best:
                best_val_mae = val_metrics['age_mae']
                patience_counter = 0
            else:
                patience_counter += 1
                
            if patience_counter >= patience:
                print(f"‚èπÔ∏è Early stopping triggered after {patience} epochs without improvement")
                break
        
        # Find and load the best model (look for any best checkpoint in the directory)
        best_checkpoints = [f for f in os.listdir(model_dir) if f.endswith('_best.pth')]
        if best_checkpoints:
            # Sort by epoch number and get the latest
            best_checkpoints.sort(key=lambda x: int(x.split('_')[2]))
            best_model_path = os.path.join(model_dir, best_checkpoints[-1])
            # FIX: Add weights_only=False for PyTorch 2.6+ compatibility
            checkpoint = torch.load(best_model_path, map_location=self.device, weights_only=False)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            print(f"üèÜ Loaded best model from: {best_model_path}")
            print(f"üéØ Best MAE: {checkpoint['val_mae']:.2f} months")
        else:
            print("‚ö†Ô∏è No best model checkpoint found, using current model state")
        
        return {
            'best_val_mae': best_val_mae,
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'val_maes': self.val_maes,
            'model_dir': model_dir
        }

class MultiDatasetTrainer(BoneAgeTrainer):
    """Extended trainer for multiple datasets with catastrophic forgetting prevention"""
    
    def __init__(self, model, device='cuda'):
        super().__init__(model, device)
        self.dataset_history = []  # Track which datasets have been used
        
    def create_combined_dataset(self, dataset_paths: List[str]) -> Tuple[List[str], List[float], List[int]]:
        """Combine multiple CSV datasets into one"""
        all_image_paths = []
        all_ages = []
        all_genders = []
        
        print("üìä Combining datasets...")
        for i, csv_path in enumerate(dataset_paths):
            if os.path.exists(csv_path):
                print(f"  Loading dataset {i+1}: {csv_path}")
                df = pd.read_csv(csv_path)
                
                image_paths = df.iloc[:, 0].tolist()
                ages_months = df.iloc[:, 1].tolist()
                genders = df.iloc[:, 2].tolist()
                
                all_image_paths.extend(image_paths)
                all_ages.extend(ages_months)
                all_genders.extend(genders)
                
                print(f"    Added {len(image_paths)} samples")
                self.dataset_history.append(csv_path)
            else:
                print(f"‚ö†Ô∏è Dataset not found: {csv_path}")
        
        print(f"‚úÖ Combined dataset: {len(all_image_paths)} total samples from {len(self.dataset_history)} datasets")
        return all_image_paths, all_ages, all_genders
    
    def create_replay_dataset(self, new_csv: str, previous_csv: str, replay_ratio: float = 0.3):
        """Create dataset that mixes new data with samples from previous dataset"""
        
        print(f"üîÑ Creating replay dataset...")
        
        # Load new dataset
        new_df = pd.read_csv(new_csv)
        new_size = len(new_df)
        print(f"  New dataset: {new_size} samples")
        
        # Load previous dataset
        prev_df = pd.read_csv(previous_csv)
        
        # Sample from previous dataset for replay
        replay_size = int(new_size * replay_ratio)
        replay_samples = prev_df.sample(n=min(replay_size, len(prev_df)), random_state=42)
        print(f"  Replay samples: {len(replay_samples)} samples ({replay_ratio*100:.1f}% of new data)")
        
        # Combine datasets
        combined_df = pd.concat([new_df, replay_samples], ignore_index=True)
        print(f"  Total training: {len(combined_df)} samples")
        
        return (combined_df.iloc[:, 0].tolist(),
                combined_df.iloc[:, 1].tolist(), 
                combined_df.iloc[:, 2].tolist())
    
    def calculate_fisher_information(self, dataloader):
        """Calculate Fisher Information Matrix for EWC"""
        print("üß† Calculating Fisher Information Matrix...")
        
        fisher_dict = {}
        optpar_dict = {}
        
        # Store current parameters
        for name, param in self.model.named_parameters():
            optpar_dict[name] = param.data.clone()
            fisher_dict[name] = torch.zeros_like(param.data)
        
        self.model.eval()
        criterion = nn.L1Loss()
        
        for batch in tqdm(dataloader, desc="Computing Fisher Information"):
            images = batch['image'].to(self.device)
            ages = batch['age_months'].to(self.device)
            
            self.model.zero_grad()
            outputs = self.model(images)
            loss = criterion(outputs['age'], ages)
            loss.backward()
            
            for name, param in self.model.named_parameters():
                if param.grad is not None:
                    fisher_dict[name] += param.grad.data ** 2
        
        # Normalize by number of samples
        for name in fisher_dict:
            fisher_dict[name] /= len(dataloader.dataset)
        
        print("‚úÖ Fisher Information Matrix calculated")
        return fisher_dict, optpar_dict
    
    def train_epoch_with_ewc(self, train_loader, optimizer, criterion_age, criterion_gender,
                            fisher_dict, optpar_dict, ewc_lambda):
        """Training epoch with EWC regularization"""
        self.model.train()
        total_loss = 0
        total_ewc_loss = 0
        total_standard_loss = 0
        
        pbar = tqdm(train_loader, desc="Training with EWC")
        for batch in pbar:
            images = batch['image'].to(self.device)
            ages = batch['age_months'].to(self.device)
            genders = batch['gender'].to(self.device)
            
            optimizer.zero_grad()
            outputs = self.model(images)
            
            # Standard losses
            age_loss = criterion_age(outputs['age'], ages)
            gender_loss = criterion_gender(outputs['gender'], genders)
            standard_loss = age_loss + 0.3 * gender_loss
            
            # EWC regularization loss
            ewc_loss = 0
            if fisher_dict:
                for name, param in self.model.named_parameters():
                    if name in fisher_dict:
                        ewc_loss += (fisher_dict[name] * 
                                   (param - optpar_dict[name]) ** 2).sum()
                ewc_loss *= ewc_lambda / 2
            
            # Total loss
            total_loss_batch = standard_loss + ewc_loss
            total_loss_batch.backward()
            
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            optimizer.step()
            
            total_loss += total_loss_batch.item()
            total_standard_loss += standard_loss.item()
            total_ewc_loss += ewc_loss.item() if isinstance(ewc_loss, torch.Tensor) else ewc_loss
            
            pbar.set_postfix({
                'Total': f'{total_loss_batch.item():.4f}',
                'Standard': f'{standard_loss.item():.4f}',
                'EWC': f'{ewc_loss.item() if isinstance(ewc_loss, torch.Tensor) else ewc_loss:.4f}'
            })
        
        return {
            'total_loss': total_loss / len(train_loader),
            'standard_loss': total_standard_loss / len(train_loader),
            'ewc_loss': total_ewc_loss / len(train_loader)
        }
    
    def fit_with_ewc(self, train_loader, val_loader, previous_model_path=None, 
                     ewc_lambda=1000, epochs=50, learning_rate=1e-4, model_name='bone_age_ewc'):
        """Train with Elastic Weight Consolidation to prevent forgetting"""
        
        # Calculate Fisher Information Matrix if we have a previous model
        fisher_dict = {}
        optpar_dict = {}
        
        if previous_model_path and os.path.exists(previous_model_path):
            print("üìÇ Loading previous model for EWC...")
            
            # Load previous model
            # FIX: Add weights_only=False for PyTorch 2.6+ compatibility
            prev_checkpoint = torch.load(previous_model_path, map_location=self.device, weights_only=False)
            self.model.load_state_dict(prev_checkpoint['model_state_dict'])
            
            # Calculate Fisher Information Matrix using some samples from training data
            fisher_dict, optpar_dict = self.calculate_fisher_information(train_loader)
        
        # Setup optimizer and criteria
        optimizer = optim.AdamW(self.model.parameters(), lr=learning_rate)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)
        criterion_age = nn.L1Loss()
        criterion_gender = nn.BCELoss()
        
        best_val_mae = float('inf')
        patience_counter = 0
        
        # Create model directory
        model_dir = f'models/{model_name}'
        os.makedirs(model_dir, exist_ok=True)
        
        print(f"üß† Starting EWC training for {epochs} epochs...")
        print(f"üìÅ EWC model checkpoints will be saved to: {model_dir}")
        print(f"üîó EWC Lambda: {ewc_lambda}")
        
        for epoch in range(epochs):
            print(f"\nEpoch {epoch + 1}/{epochs}")
            print("-" * 30)
            
            # Training with EWC regularization
            train_metrics = self.train_epoch_with_ewc(
                train_loader, optimizer, criterion_age, criterion_gender,
                fisher_dict, optpar_dict, ewc_lambda
            )
            
            # Validation (same as before)
            val_metrics = self.validate(val_loader, criterion_age, criterion_gender)
            scheduler.step(val_metrics['val_loss'])
            
            # Print metrics
            print(f"Train Loss: {train_metrics['total_loss']:.4f}")
            print(f"  Standard Loss: {train_metrics['standard_loss']:.4f}")
            print(f"  EWC Loss: {train_metrics['ewc_loss']:.4f}")
            print(f"Val MAE: {val_metrics['age_mae']:.2f} months")
            print(f"Val R¬≤: {val_metrics['age_r2']:.4f}")
            print(f"Current LR: {optimizer.param_groups[0]['lr']:.6f}")
            
            # Save checkpoint
            checkpoint_path = f'{model_dir}/ewc_checkpoint_epoch_{epoch+1}.pth'
            is_best = val_metrics['age_mae'] < best_val_mae
            
            checkpoint = {
                'model_state_dict': self.model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'epoch': epoch,
                'val_mae': val_metrics['age_mae'],
                'fisher_dict': fisher_dict,
                'optpar_dict': optpar_dict,
                'ewc_lambda': ewc_lambda,
                'train_metrics': train_metrics,
                'val_metrics': val_metrics,
                'timestamp': datetime.now().isoformat()
            }
            
            torch.save(checkpoint, checkpoint_path)
            
            if is_best:
                best_val_mae = val_metrics['age_mae']
                patience_counter = 0
                # Save best model
                best_path = checkpoint_path.replace('.pth', '_best.pth')
                shutil.copy2(checkpoint_path, best_path)
                print(f"üíæ New best EWC model saved! MAE: {best_val_mae:.2f}")
            else:
                patience_counter += 1
                
            if patience_counter >= 10:
                print("‚èπÔ∏è Early stopping triggered")
                break
        
        return {
            'best_val_mae': best_val_mae,
            'model_dir': model_dir,
            'fisher_dict': fisher_dict,
            'optpar_dict': optpar_dict
        }

class BoneAgePredictor:
    """Inference class for bone age prediction"""
    
    def __init__(self, model_path: str, device='cuda'):
        self.device = device
        self.model = None
        self.load_model(model_path)
        
        # Preprocessing for inference
        self.transform = A.Compose([
            A.Resize(512, 512),
            A.Normalize(mean=[0.485], std=[0.229]),
            ToTensorV2()
        ])
    
    def load_model(self, model_path: str):
        """Load trained model"""
        self.model = BoneAgeModel()
        # FIX: Add weights_only=False for PyTorch 2.6+ compatibility
        checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.to(self.device)
        self.model.eval()
        print(f"üìÇ Model loaded from {model_path}")
        
        # Print model info if available
        if 'timestamp' in checkpoint:
            print(f"   Trained on: {checkpoint['timestamp']}")
        if 'val_mae' in checkpoint:
            print(f"   Best validation MAE: {checkpoint['val_mae']:.2f} months")
    
    def predict_single_image(self, image_path: str, monte_carlo_samples=10) -> BoneAgeResult:
        """Predict bone age for a single image with uncertainty estimation"""
        
        # Load and preprocess image
        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
        if image is None:
            raise ValueError(f"Could not load image: {image_path}")
        
        # Convert to RGB
        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)
        
        # Apply preprocessing
        transformed = self.transform(image=image)
        image_tensor = transformed['image'].unsqueeze(0).to(self.device)
        
        predictions = []
        uncertainties = []
        
        with torch.no_grad():
            # Monte Carlo dropout for uncertainty estimation
            for _ in range(monte_carlo_samples):
                # Enable dropout during inference for uncertainty
                self.model.train()  # This enables dropout
                output = self.model(image_tensor)
                predictions.append(output['age'].cpu().item())
                uncertainties.append(output['uncertainty'].cpu().item())
        
        # Calculate statistics
        pred_mean = np.mean(predictions)
        pred_std = np.std(predictions)
        uncertainty_mean = np.mean(uncertainties)
        
        # Combine epistemic (model) and aleatoric (data) uncertainty
        total_uncertainty = np.sqrt(pred_std**2 + uncertainty_mean**2)
        
        # Calculate confidence (inverse of uncertainty)
        confidence = 1.0 / (1.0 + total_uncertainty / 12.0)  # Normalize by 12 months
        
        # Age range based on uncertainty
        age_range_min = max(0, pred_mean - 1.96 * total_uncertainty)
        age_range_max = pred_mean + 1.96 * total_uncertainty
        
        return BoneAgeResult(
            predicted_age_months=pred_mean,
            confidence=confidence,
            age_range_min=age_range_min,
            age_range_max=age_range_max,
            uncertainty=total_uncertainty
        )
    
    def predict_batch(self, image_paths: List[str]) -> List[BoneAgeResult]:
        """Predict bone age for multiple images"""
        results = []
        for image_path in tqdm(image_paths, desc="Predicting"):
            try:
                result = self.predict_single_image(image_path)
                results.append(result)
            except Exception as e:
                print(f"‚ùå Error processing {image_path}: {e}")
                results.append(None)
        return results

def create_demo_dataset(csv_path: str = None) -> Tuple[List[str], List[float], List[int]]:
    """Create or load dataset for training"""
    
    if csv_path and os.path.exists(csv_path):
        # Load from RSNA dataset or custom CSV
        df = pd.read_csv(csv_path)
        
        # Expected columns: 'image_path', 'boneage' (in months), 'male' (0/1)
        image_paths = df.iloc[:, 0].tolist()  # First column (image paths)
        ages_months = df.iloc[:, 1].tolist()  # Second column (ages)
        genders = df.iloc[:, 2].tolist()      # Third column (gender)
        return image_paths, ages_months, genders
    
    else:
        # Demo with synthetic data (replace with real dataset)
        print("‚ö†Ô∏è No dataset CSV provided. Creating demo dataset structure...")
        print("For real training, you need:")
        print("1. CSV file with columns: 'image_path', 'boneage', 'male'")
        print("2. X-ray images in the specified paths")
        
        # Return empty lists for demo
        return [], [], []

def plot_training_history(train_losses: List[float], val_losses: List[float], val_maes: List[float], save_path: str = None):
    """Plot training history"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
    
    # Loss plot
    ax1.plot(train_losses, label='Training Loss', color='blue')
    ax1.plot(val_losses, label='Validation Loss', color='red')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.set_title('Training and Validation Loss')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # MAE plot
    ax2.plot(val_maes, label='Validation MAE', color='orange', linewidth=2)
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('MAE (months)')
    ax2.set_title('Validation Mean Absolute Error')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"üìä Training history saved to {save_path}")
    
    plt.show()

def evaluate_model(predictor: BoneAgePredictor, test_image_paths: List[str], 
                  test_ages: List[float]) -> Dict:
    """Evaluate model performance"""
    print("üìä Evaluating model performance...")
    
    predictions = []
    uncertainties = []
    confidences = []
    
    for image_path in tqdm(test_image_paths, desc="Evaluating"):
        try:
            result = predictor.predict_single_image(image_path)
            predictions.append(result.predicted_age_months)
            uncertainties.append(result.uncertainty)
            confidences.append(result.confidence)
        except Exception as e:
            print(f"‚ùå Error predicting {image_path}: {e}")
            predictions.append(np.nan)
            uncertainties.append(np.nan)
            confidences.append(np.nan)
    
    # Remove NaN values
    valid_indices = ~np.isnan(predictions)
    predictions = np.array(predictions)[valid_indices]
    test_ages = np.array(test_ages)[valid_indices]
    uncertainties = np.array(uncertainties)[valid_indices]
    confidences = np.array(confidences)[valid_indices]
    
    # Calculate metrics
    mae = mean_absolute_error(test_ages, predictions)
    r2 = r2_score(test_ages, predictions)
    rmse = np.sqrt(np.mean((predictions - test_ages)**2))
    
    # Clinical accuracy (within different thresholds)
    within_6_months = np.mean(np.abs(predictions - test_ages) <= 6) * 100
    within_12_months = np.mean(np.abs(predictions - test_ages) <= 12) * 100
    within_24_months = np.mean(np.abs(predictions - test_ages) <= 24) * 100
    
    results = {
        'mae': mae,
        'rmse': rmse,
        'r2': r2,
        'within_6_months': within_6_months,
        'within_12_months': within_12_months,
        'within_24_months': within_24_months,
        'mean_uncertainty': np.mean(uncertainties),
        'mean_confidence': np.mean(confidences),
        'predictions': predictions,
        'targets': test_ages,
        'uncertainties': uncertainties
    }
    
    print(f"üìà Test Results:")
    print(f"  MAE: {mae:.2f} months")
    print(f"  RMSE: {rmse:.2f} months")
    print(f"  R¬≤: {r2:.4f}")
    print(f"  Accuracy (¬±6 months): {within_6_months:.1f}%")
    print(f"  Accuracy (¬±12 months): {within_12_months:.1f}%")
    print(f"  Accuracy (¬±24 months): {within_24_months:.1f}%")
    print(f"  Mean uncertainty: {np.mean(uncertainties):.2f} months")
    print(f"  Mean confidence: {np.mean(confidences):.3f}")
    
    return results

# Multi-dataset training strategies
def train_multiple_datasets_strategy1_combined(dataset_csvs: List[str], config: Dict):
    """Strategy 1: Combine all datasets and train once"""
    print("üîÑ Strategy 1: Combined Dataset Training")
    
    trainer = MultiDatasetTrainer(BoneAgeModel(backbone=config['backbone']))
    
    # Combine all datasets
    all_paths, all_ages, all_genders = trainer.create_combined_dataset(dataset_csvs)
    
    if not all_paths:
        print("‚ùå No datasets loaded")
        return None
    
    # Split combined dataset
    train_paths, test_paths, train_ages, test_ages, train_genders, test_genders = train_test_split(
        all_paths, all_ages, all_genders, test_size=0.2, random_state=42
    )
    
    train_paths, val_paths, train_ages, val_ages, train_genders, val_genders = train_test_split(
        train_paths, train_ages, train_genders, test_size=0.2, random_state=42
    )
    
    print(f"üìä Combined Dataset Split:")
    print(f"  Training: {len(train_paths)} images")
    print(f"  Validation: {len(val_paths)} images")
    print(f"  Test: {len(test_paths)} images")
    
    # Create dataloaders
    train_dataset = BoneAgeDataset(train_paths, train_ages, train_genders, is_training=True)
    val_dataset = BoneAgeDataset(val_paths, val_ages, val_genders, is_training=False)
    
    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=4)
    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=4)
    
    # Train on combined dataset
    results = trainer.fit(
        train_loader, val_loader, 
        epochs=config['epochs'],
        learning_rate=config['learning_rate'],
        model_name='combined_dataset_model'
    )
    
    # Plot training history
    plot_path = f"{results['model_dir']}/training_history.png"
    plot_training_history(results['train_losses'], results['val_losses'], results['val_maes'], plot_path)
    
    print("‚úÖ Combined model training completed!")
    return results

def train_multiple_datasets_strategy2_sequential(dataset_csvs: List[str], config: Dict, replay_ratio: float = 0.3):
    """Strategy 2: Sequential training with experience replay"""
    print("üîÑ Strategy 2: Sequential Training with Experience Replay")
    
    trainer = MultiDatasetTrainer(BoneAgeModel(backbone=config['backbone']))
    results_history = []
    
    for i, csv_path in enumerate(dataset_csvs):
        print(f"\nüìö Training on Dataset {i+1}: {os.path.basename(csv_path)}")
        
        if i == 0:
            # First dataset - normal training
            paths, ages, genders = create_demo_dataset(csv_path)
        else:
            # Subsequent datasets - use replay
            paths, ages, genders = trainer.create_replay_dataset(
                csv_path, dataset_csvs[i-1], replay_ratio
            )
        
        if not paths:
            print(f"‚ö†Ô∏è Skipping {csv_path} - no data loaded")
            continue
        
        # Split dataset
        train_paths, val_paths, train_ages, val_ages, train_genders, val_genders = train_test_split(
            paths, ages, genders, test_size=0.2, random_state=42
        )
        
        print(f"  Training: {len(train_paths)}, Validation: {len(val_paths)} samples")
        
        # Create dataloaders
        train_dataset = BoneAgeDataset(train_paths, train_ages, train_genders, is_training=True)
        val_dataset = BoneAgeDataset(val_paths, val_ages, val_genders, is_training=False)
        
        train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=4)
        val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=4)
        
        # Resume from previous model if not first dataset
        resume_from = None
        if i > 0 and results_history:
            last_model_dir = results_history[-1]['model_dir']
            # Find the best checkpoint from previous training
            best_checkpoints = [f for f in os.listdir(last_model_dir) if f.endswith('_best.pth')]
            if best_checkpoints:
                resume_from = os.path.join(last_model_dir, best_checkpoints[0])
        
        model_name = f'sequential_dataset_{i+1}_replay'
        results = trainer.fit(
            train_loader, val_loader, 
            epochs=config['epochs']//2,  # Fewer epochs for sequential training
            learning_rate=config['learning_rate'] * 0.5,  # Lower learning rate for fine-tuning
            resume_from=resume_from,
            model_name=model_name
        )
        
        results['dataset_name'] = os.path.basename(csv_path)
        results_history.append(results)
        
        print(f"‚úÖ Dataset {i+1} training completed. Best MAE: {results['best_val_mae']:.2f}")
    
    print("üéâ Sequential training with replay completed!")
    return results_history

def train_multiple_datasets_strategy3_ewc(dataset_csvs: List[str], config: Dict, ewc_lambda: float = 1000):
    """Strategy 3: Sequential training with Elastic Weight Consolidation"""
    print("üîÑ Strategy 3: Sequential Training with EWC")
    
    trainer = MultiDatasetTrainer(BoneAgeModel(backbone=config['backbone']))
    results_history = []
    
    for i, csv_path in enumerate(dataset_csvs):
        print(f"\nüß† Training on Dataset {i+1} with EWC: {os.path.basename(csv_path)}")
        
        paths, ages, genders = create_demo_dataset(csv_path)
        
        if not paths:
            print(f"‚ö†Ô∏è Skipping {csv_path} - no data loaded")
            continue
        
        # Split dataset
        train_paths, val_paths, train_ages, val_ages, train_genders, val_genders = train_test_split(
            paths, ages, genders, test_size=0.2, random_state=42
        )
        
        # Create dataloaders
        train_dataset = BoneAgeDataset(train_paths, train_ages, train_genders, is_training=True)
        val_dataset = BoneAgeDataset(val_paths, val_ages, val_genders, is_training=False)
        
        train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=4)
        val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=4)
        
        # Use EWC if not first dataset
        previous_model = None
        if i > 0 and results_history:
            last_model_dir = results_history[-1]['model_dir']
            # Find the best EWC checkpoint from previous training
            best_checkpoints = [f for f in os.listdir(last_model_dir) if f.endswith('_best.pth')]
            if best_checkpoints:
                previous_model = os.path.join(last_model_dir, best_checkpoints[0])
        
        model_name = f'ewc_dataset_{i+1}'
        results = trainer.fit_with_ewc(
            train_loader, val_loader, 
            previous_model_path=previous_model,
            ewc_lambda=ewc_lambda,
            epochs=config['epochs']//2,
            learning_rate=config['learning_rate'] * 0.5,
            model_name=model_name
        )
        
        results['dataset_name'] = os.path.basename(csv_path)
        results_history.append(results)
        
        print(f"‚úÖ EWC Dataset {i+1} completed. Best MAE: {results['best_val_mae']:.2f}")
    
    print("üéâ Sequential training with EWC completed!")
    return results_history

def main():
    """Main training and evaluation pipeline - with CSV file selection"""
    
    # Get CSV file from user
    print("üìÑ Enter the training CSV file path:")
    print("   (Type the path)")
    csv_file = input("CSV file: ").strip()
    
    # Remove quotes if file was drag-and-dropped
    csv_file = csv_file.strip('"').strip("'")
    
    # Default to bone_age_dataset.csv if nothing entered
    if not csv_file:
        csv_file = "bone_age_dataset.csv"
        print(f"üìÑ Using default: {csv_file}")
    
    # Validate file exists
    if not os.path.exists(csv_file):
        print(f"‚ùå File not found: {csv_file}")
        print("Please check the file path and try again.")
        return
    
    # Configuration - simplified with auto-resume
    config = {
        'data_csv': csv_file,  # User-specified CSV file
        'batch_size': 16,
        'epochs': 50,
        'learning_rate': 1e-4,
        'backbone': 'efficientnet_b3',
        'device': 'cuda' if torch.cuda.is_available() else 'cpu',
        'resume_from': 'best_bone_age_model.pth' if os.path.exists('best_bone_age_model.pth') else None,
        'start_fresh': False
    }
    
    print(f"üñ•Ô∏è Using device: {config['device']}")
    print(f"üìÑ Training data: {csv_file}")
    
    # Check for existing model
    if config['resume_from']:
        print(f"üîç Found existing model: {config['resume_from']}")
        print("üìö Will continue training from checkpoint")
    else:
        print("üÜï No existing model found, starting fresh training")
    
    # Load dataset
    image_paths, ages_months, genders = create_demo_dataset(config['data_csv'])
    
    if not image_paths:
        print(f"‚ùå No dataset loaded from: {csv_file}")
        print("Please ensure the CSV file has the correct format:")
        print("  Column 1: image_path")
        print("  Column 2: boneage (in months)")
        print("  Column 3: gender (0=female, 1=male)")
        print("\nExample usage after training:")
        print("""
# Load trained model for inference:
predictor = BoneAgePredictor('best_bone_age_model.pth')
result = predictor.predict_single_image('path/to/xray.jpg')
print(f"Predicted age: {result.predicted_age_months:.1f} months")
print(f"Confidence: {result.confidence:.3f}")
        """)
        return
    
    # Split dataset
    train_paths, test_paths, train_ages, test_ages, train_genders, test_genders = train_test_split(
        image_paths, ages_months, genders, test_size=0.2, random_state=42, stratify=genders
    )
    
    train_paths, val_paths, train_ages, val_ages, train_genders, val_genders = train_test_split(
        train_paths, train_ages, train_genders, test_size=0.2, random_state=42, stratify=train_genders
    )
    
    print(f"üìä Dataset split:")
    print(f"  Training: {len(train_paths)} images")
    print(f"  Validation: {len(val_paths)} images")
    print(f"  Test: {len(test_paths)} images")
    
    # Create datasets and dataloaders
    train_dataset = BoneAgeDataset(train_paths, train_ages, train_genders, is_training=True)
    val_dataset = BoneAgeDataset(val_paths, val_ages, val_genders, is_training=False)
    
    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=4)
    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=4)
    
    # Create model
    model = BoneAgeModel(backbone=config['backbone'])
    trainer = BoneAgeTrainer(model, device=config['device'])
    
    # Train model with auto-resume
    training_results = trainer.fit(
        train_loader, 
        val_loader, 
        epochs=config['epochs'],
        learning_rate=config['learning_rate'],
        resume_from=config['resume_from'],
        model_name='bone_age_model'
    )
    
    # Plot training history
    plot_path = f"{training_results['model_dir']}/training_history.png"
    plot_training_history(
        training_results['train_losses'],
        training_results['val_losses'],
        training_results['val_maes'],
        plot_path
    )
    
    # Find best model checkpoint
    model_dir = training_results['model_dir']
    best_checkpoints = [f for f in os.listdir(model_dir) if f.endswith('_best.pth')]
    if best_checkpoints:
        best_model_path = os.path.join(model_dir, best_checkpoints[0])
        
        # Evaluate on test set
        predictor = BoneAgePredictor(best_model_path, device=config['device'])
        test_results = evaluate_model(predictor, test_paths, test_ages)
        
        # Save comprehensive results
        results_summary = {
            'config': config,
            'training_results': {k: v for k, v in training_results.items() if k != 'model_dir'},
            'test_results': {k: v for k, v in test_results.items() if k not in ['predictions', 'targets', 'uncertainties']},
            'model_path': best_model_path,
            'timestamp': datetime.now().isoformat()
        }
        
        results_path = f"{model_dir}/complete_results.json"
        with open(results_path, 'w') as f:
            json.dump(results_summary, f, indent=2)
        
        print(f"\n‚úÖ Training completed!")
        print(f"üèÜ Best model saved as: best_bone_age_model.pth")
        print(f"üìä Detailed results: {results_path}")
        print(f"üìà Training plots: {plot_path}")
        print(f"üéØ Best validation MAE: {training_results['best_val_mae']:.2f} months")
    else:
        print("‚ö†Ô∏è No best model checkpoint found")

if __name__ == "__main__":
    print("ü¶¥ Bone Age Prediction System")
    print("="*40)
    print("üöÄ Interactive training with CSV file selection")
    print("üîç Auto-resume from: best_bone_age_model.pth (if exists)")
    print("üìÇ Default CSV: bone_age_dataset.csv (if no input)")
    print()
    
    main()